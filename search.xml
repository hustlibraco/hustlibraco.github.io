<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[虞美人· 听雨]]></title>
    <url>%2F2017%2F07%2F21%2Fyumeiren%2F</url>
    <content type="text"><![CDATA[宋·蒋捷 少年听雨歌楼上，红烛昏罗帐。壮年听雨客舟中，江阔云低，断雁叫西风。而今听雨僧庐下，鬓已星星也。悲欢离合总无凭，一任阶前点滴到天明。]]></content>
      <categories>
        <category>诗</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TK对安全从业者的成长建议]]></title>
    <url>%2F2017%2F06%2F30%2Fsecurity-personal-development%2F</url>
    <content type="text"><![CDATA[今天看到一条TK的微博，分享了腾讯玄武实验室内部例会的一则PPT，讲的是个人成长。觉得讲得很好，在这里总结一下。 个人成长确立个人方向，结合工作内容，找出对应短板 该领域主要专家们的工作是否都了解？ 相关网络协议、文件格式是否都熟悉？ 相关的技术和主要工具是否看过、用过？ 阅读只是学习过程的起点，不能止于阅读 工具的每个参数每个菜单都要看、要试 学习网络协议要实际抓包分析、学习文件格式要读代码实现 学习老漏洞一定要调试，搞懂别人代码每一个字节的意义，之后一定要完全自己重写一个Exploit 细节、细节、细节，刨根问底 建立学习参考目标短期参考：比自己优秀的同龄人 阅读他们的文章和其他工作成果，从细节中观察他们的学习方式和工作方式 中期参考：相同方向上的业内专家 了解他们的成长轨迹，跟踪他们关注的内容 长期参考：业内老牌企业和先锋企业 把握行业发展、技术趋势，为未来做积累 推荐的学习方式以工具为线索 Kali Linux 以Metasploit为例： 遍历每个子目录，除了Exploit里面还有什么？ 每个工具分别有什么功能？原理是什么？涉及哪些知识？ 能否改进优化？能否发展、组合出新的功能？ 以专家为线索 你的技术方向里有哪些专家？ 他们的邮箱、主页、社交网络账号是什么？ 他们在该方向上有哪些作品？发表过哪些演讲？ 处理好学习、工作和生活学习、工作和生活是矛盾统一的 三者都需要时间，你一天只有24个小时 调和矛盾的关键：提高效率 对于一个没有好爸爸的人来说，学习、工作决定你能不能追求远方 如何提高效率做好预研，收集相关前人成果，避免无谓的重复劳动 在可行性判断阶段，能找到工具就不写代码，能用脚本语言写就不用编译语言，把完美主义放在最终实现阶段 做好笔记并定期整理，遗忘会让所有的投入都白白浪费 多和同事交流，别人说一个工具的名字可能让你节约数小时 咖啡可以提高思维效率，而且合法（？） 无论怎么提高效率，要成为专家，都需要大量的时间投入]]></content>
      <categories>
        <category>生活感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[安全快速地导出Redis集合大数据]]></title>
    <url>%2F2017%2F04%2F05%2Fdump-huge-redis-data%2F</url>
    <content type="text"><![CDATA[最近利用redis的集合做数据去重，共存储3e条记录到一个键。现在需要把这3e数据导出到文本文件里面来。首先我们不能用smembers方法，这个方法会阻塞Redis进程，由于数据量太多，可能会阻塞几十秒，排除这个方法。 然后我们想到mysql导出大数据的时候会利用cursor来迭代处理，查阅redis文档，我们发现有类似的方法sscan, 可以一次取出指定数量的数据，并且迭代集合。这样不会阻塞redis服务，而且速度很快（40min处理了3e条数据）。 SCAN cursor [MATCH pattern] [COUNT count] Available since 2.8.0.Time complexity: O(1) for every call. O(N) for a complete iteration, including enough command calls for the cursor to return back to 0. N is the number of elements inside the collection. 代码片段如下： def main(): with open(to, 'wb') as f: cursor = '0' while cursor != 0: cursor, data = r.sscan(key, cursor=cursor, count=count) data = ['&#123;0&#125;\n'.format(i) for i in data] # 批量写入减少IO操作 f.writelines(data) print('cursor:&#123;0&#125;'.format(cursor)) 迭代开始的时候我们用0初始化cursor的值，每次迭代的时候都传入上一轮返回cursor值，直到cursor再次等于0的时候遍历结束。 sscan方法有几个特别的地方： 返回的cursor值并不是递增的，而且随机变化的，也就是说你不能通过cursor判断当前集合遍历的进度。 某些元素可能会被返回多次。 A given element may be returned multiple times. It is up to the application to handle the case of duplicated elements, for example only using the returned elements in order to perform operations that are safe when re-applied multiple times. 当集合在迭代的时候发生变化，某些元素可能不会被返回。 Elements that were not constantly present in the collection during a full iteration, may be returned or not: it is undefined. 迭代返回元素的数量可能和count指定的不一致 While SCAN does not provide guarantees about the number of elements returned at every iteration, it is possible to empirically adjust the behavior of SCAN using the COUNT option. Basically with COUNT the user specified the amount of work that should be done at every call in order to retrieve elements from the collection. This is just an hint for the implementation, however generally speaking this is what you could expect most of the times from the implementation. The default COUNT value is 10. When iterating the key space, or a Set, Hash or Sorted Set that is big enough to be represented by a hash table, assuming no MATCH option is used, the server will usually return count or a bit more than count elements per call. When iterating Sets encoded as intsets (small sets composed of just integers), or Hashes and Sorted Sets encoded as ziplists (small hashes and sets composed of small individual values), usually all the elements are returned in the first SCAN call regardless of the COUNT value. Important: there is no need to use the same COUNT value for every iteration. The caller is free to change the count from one iteration to the other as required, as long as the cursor passed in the next call is the one obtained in the previous call to the command. 因此不建议在集合可能会发生变动的情况下使用这个方法。 参考链接： redis官方文档 如何优雅地删除Redis大键]]></content>
      <categories>
        <category>开发</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用virtualenvwrapper(Linux + Windows)]]></title>
    <url>%2F2017%2F01%2F10%2Fuse-virtualenvwrapper%2F</url>
    <content type="text"><![CDATA[生活的问题在于，你永远不知道下一个到来的是什么问题。开发亦然。之前Python都是用一个运行环境，偶尔用一下virtualenv，直到开始在windows调试python才不得不来学习virtualenvwrapper。 简介Virtualenvwrapper解决的主要问题是和virtualenv一样的，但是它弥补了virtualenv无法统一管理环境的缺点。 安装virtualenvwrapper安装方式还是比较简单的。 Linux1.安装pip install virtualenvwrapper 2.设置环境变量export WORKON_HOME=~/Envsexport PROJECT_HOME=$HOME/pyprojsource /usr/bin/virtualenvwrapper.sh 建议添加到自启动脚本中~/.bashrc 或 ~/.profile。 Windowswindows下直接安装virtualenvwrapper似乎不能运行。我搜索到有独立的安装包，也能通过pip安装。https://github.com/davidmarble/virtualenvwrapper-win。 1.安装# using pippip install virtualenvwrapper-win# using easy_installeasy_install virtualenvwrapper-win# from sourcegit clone git://github.com/davidmarble/virtualenvwrapper-win.gitcd virtualenvwrapper-winpython setup.py install 2.设置环境变量（可选）Optional: Add an environment variable WORKON_HOME to specify the path to store environments. By default, this is `%USERPROFILE%\Envs`。 3.可能出现的错误 File &quot;D:\Python\Python27\Lib\mimetypes.py&quot;, line 249, in enum_types ctype = ctype.encode(default_encoding) # omit in 3.x!UnicodeDecodeError: &apos;ascii&apos; codec can&apos;t decode byte 0xb0 in position 1: ordinal not in range(128) 原因是某些程序（阿里旺旺）写了一些中文键值到注册表里。具体看http://pcliuyang.blog.51cto.com/8343567/1339637 解决方法：打开Python27/Lib/mimetypes.py，找到default_encoding = sys.getdefaultencoding()一行，在它前面添加如下代码：# begin fix bugif sys.getdefaultencoding() != &apos;gbk&apos;: reload(sys) sys.setdefaultencoding(&apos;gbk&apos;)# end 命令主要命令Linux和windows是一致的，部分有出入。 主要命令mkvirtualenv &lt;name&gt; Create a new virtualenv environment named . The environment will be created in WORKON_HOME. lsvirtualenv List all of the enviornments stored in WORKON_HOME. rmvirtualenv &lt;name&gt; Remove the environment . Uses folder_delete.bat. workon [&lt;name&gt;] If is specified, activate the environment named (change the working virtualenv to ). If a project directory has been defined, we will change into it. If no argument is specified, list the available environments. One can pass additional option -c after virtualenv name to cd to virtualenv directory if no projectdir is set. deactivate Deactivate the working virtualenv and switch back to the default system Python. add2virtualenv &lt;full or relative path&gt; If a virtualenv environment is active, appends to virtualenv_path_extensions.pth inside the environment’s site-packages, which effectively adds to the environment’s PYTHONPATH. If a virtualenv environment is not active, appends to virtualenv_path_extensions.pth inside the default Python’s site-packages. If doesn’t exist, it will be created. 其他命令cdproject If a virtualenv environment is active and a projectdir has been defined, change the current working directory to active virtualenv’s project directory. cd- will return you to the last directory you were in before calling cdproject. cdsitepackages If a virtualenv environment is active, change the current working directory to the active virtualenv’s site-packages directory. If a virtualenv environment is not active, change the current working directory to the default Python’s site-packages directory. cd- will return you to the last directory you were in before calling cdsitepackages. cdvirtualenv If a virtualenv environment is active, change the current working directory to the active virtualenv base directory. If a virtualenv environment is not active, change the current working directory to the base directory of the default Python. cd- will return you to the last directory you were in before calling cdvirtualenv. lssitepackages If a virtualenv environment is active, list that environment’s site-packages. If a virtualenv environment is not active, list the default Python’s site-packages. Output includes a basic listing of the site-packages directory, the contents of easy-install.pth, and the contents of virtualenv_path_extensions.pth (used by add2virtualenv). setprojectdir(win)/setvirtualenvproject(Linux) 这两个命令功能相同，但是格式不同。setprojectdir只需要一个参数，指定工程主目录。setvirtualenvproject需要两个参数，参数一指定虚拟环境目录，参数二指定工程主目录。参数可省略，省略时使用当前的虚拟环境和当前目录。 # setvirtualenvproject 用法示例$ mkproject myprojNew python executable in myproj/bin/pythonInstalling setuptools.................................................................................................................................................................................done.Creating /Users/dhellmann/Devel/myproj(myproj)$ mkvirtualenv myproj_new_libsNew python executable in myproj/bin/pythonInstalling setuptools.................................................................................................................................................................................done.Creating /Users/dhellmann/Devel/myproj(myproj_new_libs)$ setvirtualenvproject $VIRTUAL_ENV $(pwd) toggleglobalsitepackages If a virtualenv environment is active, toggle between having the global site-packages in the PYTHONPATH or just the virtualenv’s site-packages. whereis &lt;file&gt; A script included for convenience. Returns directory locations of file and file with any executable extensions. So you can call whereis python to find all executables starting with python or whereis python.exe for an exact match. mkproject(Only in Linux) 项目将创建到PROJECT_HOME目录下，实际上相当于在某个目录下，建了一个环境。]]></content>
      <categories>
        <category>开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建使用个人Git服务器]]></title>
    <url>%2F2016%2F12%2F16%2Fprivate-git%2F</url>
    <content type="text"><![CDATA[一直一来都不用IDE，直接用SFTPDriver把Linux服务器文件直接映射到Windows本次操作。可惜Win10已经用不了SftpDriver，替代品稳定性捉急，因此在Linux上搭建个人Git服务器成为了必然。 Git服务器搭建以CentOS7为例，有以下几个步骤。 安装gitsudo yum install git 创建git用户，创建authorized_keys文件一定要设置好权限，否则无法起作用或者不安全adduser git su - gitmkdir .ssh &amp;&amp; chmod 700 .sshtouch .ssh/authorized_keyschmod 600 .ssh/authorized_keys 创建一个空仓库cd /home/gitgit init --bare sample.git 禁止git用户登录服务器# 修改 /bin/bash 为 git-shellvim /etc/passwdgit:x:1001:1002::/home/git:/usr/bin/git-shell 退出git用户TortoiseGit客户端使用使用Puttygen生成密钥对公钥复制粘贴到服务器的/home/git/.ssh/authorized_keys文件中。私钥保存在本地。 TortoiseGit默认不是OpenSSH 克隆仓库右键 git clone，加载Putty密钥选择刚刚生成的私钥。 错误帮助如果提示网络无法链接，那么服务器的ssh端口很可能不是22！如果服务器ssh端口不是22，则要求如下格式： ssh://git@115.28.137.182:23456/home/git/sample.git ssh:// 前缀必不可少 ip和端口之间一个冒号，端口和项目地址没有冒号！ 如果要求你输入git的密码，那么很有可能authorized_keys有问题 格式错误 权限不对 具体可以在 systemctl status sshd 里看到错误日志！]]></content>
      <categories>
        <category>开发</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】Python装饰器]]></title>
    <url>%2F2016%2F12%2F05%2Fpython-decorator%2F</url>
    <content type="text"><![CDATA[装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 先来看一个简单例子： def foo(): print('i am foo')现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码：def foo(): print('i am foo') logging.info("foo is running") bar()、bar2()也有类似的需求，怎么做？再写一个logging在bar函数里？这样就造成大量雷同的代码，为了减少重复写代码，我们可以这样做，重新定义一个函数：专门处理日志 ，日志处理完之后再执行真正的业务代码 def use_logging(func): logging.warn("%s is running" % func.__name__) func()def bar(): print('i am bar')use_logging(bar) 逻辑上不难理解， 但是这样的话，我们每次都要将一个函数作为参数传递给use_logging函数。而且这种方式已经破坏了原有的代码逻辑结构，之前执行业务逻辑时，执行运行bar()，但是现在不得不改成use_logging(bar)。那么有没有更好的方式的呢？当然有，答案就是装饰器。 简单装饰器def use_logging(func): def wrapper(*args, **kwargs): logging.warn("%s is running" % func.__name__) return func(*args, **kwargs) return wrapperdef bar(): print('i am bar')bar = use_logging(bar)bar() 函数use_logging就是装饰器，它把执行真正业务方法的func包裹在函数里面，看起来像bar被use_logging装饰了。在这个例子中，函数进入和退出时，被称为一个横切面(Aspect)，这种编程方式被称为面向切面的编程(Aspect-Oriented Programming)。@符号是装饰器的语法糖，在定义函数的时候使用，避免再一次赋值操作。 def use_logging(func): def wrapper(*args, **kwargs): logging.warn("%s is running" % func.__name__) return func(*args) return wrapper@use_loggingdef foo(): print("i am foo")@use_loggingdef bar(): print("i am bar")bar() 如上所示，这样我们就可以省去bar = use_logging(bar)这一句了，直接调用bar()即可得到想要的结果。如果我们有其他的类似函数，我们可以继续调用装饰器来修饰函数，而不用重复修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。 装饰器在Python使用如此方便都要归因于Python的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他变量，可以作为返回值，可以被定义在另外一个函数内。 带参数的装饰器装饰器还有更大的灵活性，例如带参数的装饰器：在上面的装饰器调用中，比如@use_logging，该装饰器唯一的参数就是执行业务的函数。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。 def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == "warn": logging.warn("%s is running" % func.__name__) return func(*args) return wrapper return decorator@use_logging(level="warn")def foo(name='foo'): print("i am %s" % name)foo() 上面的use_logging是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=”warn”)调用的时候，Python能够发现这一层的封装，并把参数传递到装饰器的环境中。 类装饰器再来看看类装饰器，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器还可以依靠类内部的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。class Foo(object): def __init__(self, func): self._func = funcdef __call__(self): print ('class decorator runing') self._func() print ('class decorator ending')@Foodef bar(): print ('bar')bar() functools.wraps使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、__name__、参数列表，先看例子：装饰器def logged(func): def with_logging(*args, **kwargs): print func.__name__ + " was called" return func(*args, **kwargs) return with_logging 函数@loggeddef f(x): """does some math""" return x + x * x 该函数完成等价于：def f(x): """does some math""" return x + x * xf = logged(f) 不难发现，函数f被with_logging取代了，当然它的docstring，__name__就是变成了with_logging函数的信息了。print f.__name__ # prints 'with_logging'print f.__doc__ # prints None 这个问题就比较严重的，好在我们有functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器函数中，这使得装饰器函数也有和原函数一样的元信息了。 from functools import wrapsdef logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ + " was called" return func(*args, **kwargs) return with_logging@loggeddef f(x): """does some math""" return x + x * xprint f.__name__ # prints 'f'print f.__doc__ # prints 'does some math' 内置装饰器@staticmathod、@classmethod、@property 装饰器的顺序@a@b@cdef f (): pass 等效于f = a(b(c(f))) 作者：zhijun liu链接：https://www.zhihu.com/question/26930016/answer/99243411来源：知乎著作权归作者所有，转载请联系作者获得授权。 个人练习#coding=utf-8#!/usr/bin/env pythonfrom functools import wrapsfrom hashlib import md5from random import randintimport osimport redisimport time, datetimeclass Decorator(object): """装饰器类""" func_cache_dict = &#123;&#125; func_cache_key = 'func_cache' rds = redis.StrictRedis() @classmethod def func_cache(cls, func): @wraps(func) def wrapper(*args, **kargs): argstr = '&#123;0&#125;+&#123;1&#125;+&#123;2&#125;'.format(func.__name__, '+'.join(str(i) for i in args), '+'.join('&#123;0&#125;=&#123;1&#125;'.format(k,v) for k,v in kargs.iteritems())) uniqid = md5(argstr).hexdigest() if uniqid not in cls.func_cache_dict: try: res = func(*args, **kargs) except Exception as e: logging.error(e, exc_info=True) return None else: cls.func_cache_dict[uniqid] = res return cls.func_cache_dict[uniqid] return wrapper @classmethod def fun_cache_expire(cls, expire_time): assert isinstance(expire_time, int), 'expire_time must be a integer.' def decorator(func): @wraps(func) def wrapper(*args, **kargs): argstr = '&#123;0&#125;+&#123;1&#125;+&#123;2&#125;'.format(func.__name__, '+'.join(str(i) for i in args), '+'.join('&#123;0&#125;=&#123;1&#125;'.format(k,v) for k,v in kargs.iteritems())) uniqid = md5(argstr).hexdigest() expire_at = float(cls.rds.get(cls.func_cache_key + uniqid) or 0) cond1 = (uniqid not in cls.func_cache_dict) cond2 = (time.time() &gt; expire_at) # print 'cond1:&#123;0&#125;, cond2:&#123;1&#125;'.format(cond1, cond2) if cond1 or cond2: try: res = func(*args, **kargs) except Exception as e: logging.error(e, exc_info=True) return None else: cls.func_cache_dict[uniqid] = res cls.rds.setex(cls.func_cache_key + uniqid, datetime.timedelta(seconds=expire_time), time.time() + expire_time) return cls.func_cache_dict[uniqid] return wrapper return decoratordef randomstr1(): return os.urandom(8).encode('hex')@Decorator.func_cachedef randomstr2(): return os.urandom(8).encode('hex')@Decorator.fun_cache_expire(3)# 结果缓存三秒def randomstr3(): return os.urandom(8).encode('hex')@Decorator.fun_cache_expire(1)# 带参数的函数def randomstr4(*args): s = '_'.join(str(i) for i in args) return md5(s).hexdigest()for i in range(8): args = [randint(1,100) for i in range(3)] print randomstr1(), randomstr2(), randomstr3(), randomstr4(*args) time.sleep(1)print randomstr1(), randomstr2(), randomstr3(), randomstr4(*args) 运行结果：302e93d809adda2f 828719ab1a357f0a 9e9a3d8e26d8b101 71078cd8bf589b0a82a4d64867a4fca98ff79b5be99afab7 828719ab1a357f0a 9e9a3d8e26d8b101 2642897d9d2e5d403790f5efcb51c9de284014dc432de190 828719ab1a357f0a 9e9a3d8e26d8b101 79c3b2f9cee061069dc95f27ec93893b# randomstr3 函数结果缓存3秒，所以3秒变换一次结果8d6201c551e12e08 828719ab1a357f0a bae42a9245254d39 840d506ee9004882b3b8537c722cc9178038c2c75f0e2584 828719ab1a357f0a bae42a9245254d39 454715c6f86330eebf9a79d65c4c519d1ad603c8b1c0f862 828719ab1a357f0a bae42a9245254d39 0c69a374754d0c546fe588ff638c4376f3a35c418c35084c 828719ab1a357f0a 6997bc4c1a5c6acf 71a735b4c528d6eb7bc49375f2e031a7d42fcb94450a61aa 828719ab1a357f0a 6997bc4c1a5c6acf 4d309f93de6bcb766ca623d9270cf422# randomstr4 函数最后一次执行的入参与上次相同，所以结果与上一次相同90e416f4a6177b8d 828719ab1a357f0a 6997bc4c1a5c6acf 4d309f93de6bcb766ca623d9270cf422]]></content>
      <categories>
        <category>开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip设置国内源]]></title>
    <url>%2F2016%2F11%2F28%2Fpypi-source-setting%2F</url>
    <content type="text"><![CDATA[每次学习编程的时候都会遇到许多运维的问题。身为立志成为一名全站工程师的我，只有不断折腾了。 pip的默认下载源是国外的，所以不是很快，类似yum我们想替换成国内的源，比如： 阿里云 http://mirrors.aliyun.com/pypi/simple/ 豆瓣 https://pypi.doubanio.com/simple/ 网上搜到的地址很多都是过期的，验证方法是用浏览器打开的能看到包列表的地址。比如https://pypi.douban.com/simple/用浏览器打开会跳转至https://pypi.doubanio.com/simple/，这才是最终的正确地址。 以豆瓣源为例，在Linux下面，创建/etc/pip.conf，编辑内容如下: [global]timeout = 60trusted-host=pypi.doubanio.comindex-url=https://pypi.doubanio.com/simple/ 保存即可生效，甚至包括virtualenv创建出来的pip也会用这个源。 临时用法pip install xxx -i http://mirrors.aliyun.com/pypi/simple/ 更多设置可以针对不同Linux用户设置不同源，详见 https://pip.pypa.io/en/stable/user_guide/#configuration]]></content>
      <categories>
        <category>开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根据php加密算法编写python解密代码]]></title>
    <url>%2F2016%2F10%2F09%2Fphp2python-code%2F</url>
    <content type="text"><![CDATA[服务器搬家，暂时无法工作，拿起一道CTF练练手。 php代码是一张图片: 使用了rot13、逆序、base64等方法加密，我们将其翻译成python代码： def rot13(s, offset=13): ''' ROT-13 编码是一种每一个字母被另一个字母代替的方法。 这个代替字母是由原来的字母向前移动 13 个字母而得到的。数字和非字母字符保持不变。 ''' def encodeCh(ch): f=lambda x: chr((ord(ch)-x+offset) % 26 + x) return f(97) if ch.islower() else (f(65) if ch.isupper() else ch) return ''.join(encodeCh(c) for c in s)def encode(s): oo = '' o = s[::-1] for i in range(0, len(o)): c = chr(ord(o[i])+1) oo += c return rot13(base64.b64encode(oo)[::-1]) 照着encode函数，很容易可以写出来decode函数: def decode(s): o = '' oo = base64.b64decode(rot13(s)[::-1]) for i in range(0, len(oo)): c = chr(ord(oo[i])-1) o += c return o[::-1] 密文s=a1zLbgQsCESEIqRLwuQAyMwLyq2L5VwBxqGA3RQAyumZ0tmMvSGM2ZwB4tws, 我们调用一下decode函数，直接得到结果。 root@kaliSec:~/文档# python 1.py a1zLbgQsCESEIqRLwuQAyMwLyq2L5VwBxqGA3RQAyumZ0tmMvSGM2ZwB4tws flag:&#123;NSCTF_b73d5adfb819c64603d7237fa0d52977&#125; 语言都是相通的，熟悉算法和数据结构更为关键。]]></content>
      <categories>
        <category>开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Php</tag>
        <tag>CTF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kali Linux 2016.1 安装 Chrome]]></title>
    <url>%2F2016%2F09%2F18%2Fkali-linux-install-chrome%2F</url>
    <content type="text"><![CDATA[Kali Linux 2016.1 Rolling Edtion 预装的是iceweasel浏览器，如果要使用chrome需要自己下载安装包并需要额外的设置。 1. 下载地址wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb实测不用翻墙，速度还挺快。 2. 安装命令dpkg -i google-chrome-stable_current_amd64.deb 如果缺少依赖： apt-get install -f 3. 运行设置安装完chrome之后我们发现并不能正常打开它，需要在/usr/share/application里找到chrome的快捷方式，在原先的运行命令添加--no-sandbox和--user-dir-data。 --no-sandbox 解决点击图片无响应问题 --user-dir-data 解决Chrome不能在root用户下运行的问题]]></content>
      <categories>
        <category>安全</category>
        <category>Kali</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Kali</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Gunicorn部署Flask Web服务]]></title>
    <url>%2F2016%2F08%2F31%2Fgunicorn-with-flask%2F</url>
    <content type="text"><![CDATA[FlaskFlask 虽然自带 Web 服务器，但是该服务器性能较低，是单进程单线程模型，原本是供开发测试使用。所以我们在生产环境中需要使用 Gunicorn 这样高性能服务器部署Flask服务。 GunicornGunicorn ‘Green Unicorn’ 是一个 UNIX 下的 WSGI HTTP 服务器，它是一个移植自 Ruby 的 Unicorn 项目的 pre-fork worker 模型。它既支持 eventlet，也支持 greenlet。在 Gunicorn 上运行 Flask 应用非常简单: gunicorn myproject:app Gunicorn 提供许多命令行参数，可以使用 gunicorn -h 来获得帮助。下面的例子 使用 4 worker 进程（ -w 4 ）来运行 Flask 应用，绑定到 localhost 的 4000 端口（ -b 127.0.0.1:4000 ）: gunicorn -w 4 -b 127.0.0.1:4000 myproject:app 几点注意的地方1.Gunicorn 日志分为 accesslog 和 errorlog，但是如果你没有配置这两个选项，它们不会被打印出来。 2.被 Gunicorn 包裹的web服务日志，可以改写输出到 Gunicorn errorlog 上，记得设置 errorlog 的 loglevel ，要不然你可能看不到日志。 3.如果 Gunicorn 开启了多进程，原来服务的日志系统也需要兼容多进程，或者仅输出到 Gunicorn 的 errorlog 上，写法如： app.logger.setLevel(logging.INFO)app.logger.handlers.extend(logging.getLogger("gunicorn.error").handlers) 我猜测这样是多进程安全的，仅仅是猜测! 4.如果你的服务在启动和结束之前还要执行其他操作，目前我不知道Gunicorn如何实现，官方给了自定义Gunicorn的写法，但是我遇到了无法结束的bug。参见：http://docs.gunicorn.org/en/stable/custom.html gunicorn server-hooks 提供这样的功能, 只需要在配置文件中添加对应的函数: import osimport syssys.path.append(os.path.abspath(os.path.dirname(__file__)))from spectre import cronsbind = ':5000'loglevel = 'info'access_log_format = '%(h)s %(l)s %(u)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s"'accesslog = "/dev/null"errorlog = "/dev/null"def on_starting(server): # gunicorn 主进程启动之前的操作 crons.start()def on_exit(server): # gunicorn 退出之后的操作 crons.stop() 还有其他的钩子函数，参考：http://docs.gunicorn.org/en/latest/settings.html#server-hooks 后记原来喜欢用 python 不喜欢 php 写 web 最大的原因之一就是 php 必须假设在一个web服务器上，配置麻烦。现在发现python web框架自带的服务器就是个玩具而已，生产配置是一样的麻烦。 我还是太年轻了。也许可以花时间学下php，在适合的场景用适合的语言。]]></content>
      <categories>
        <category>开发</category>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Gunicorn</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Burpsuite初探]]></title>
    <url>%2F2016%2F08%2F05%2Fburpsuite-install%2F</url>
    <content type="text"><![CDATA[简介Burp Suite是响当当的web应用程序渗透测试集成平台。从应用程序攻击表面的最初映射和分析，到寻找和利用安全漏洞等过程，所有工具为支持整体测试程序而无缝地在一起工作。 安装版本：Burp suite pro v1.6.34（破解版本）下载地址：http://download.csdn.net/download/luckchoudog/9451559安装JDK: 百度下载 官方下载 启动解压Burp suite pro v1.6.34.rar，执行BurpLoader.jar即可。 HTTPS代理抓包设置1.访问代理服务器WEB界面http://127.0.0.1:8080，如果访问不了，更改为下图设置中的地址： 2.下载证书 3.双击文件安装到 受信任的根证书颁发机构 4.导出为cer格式证书 5.安装cer证书到 受信任的根证书颁发机构 同步骤3 HTTPS代理抓包设置（Chrome in Linux）如果是在Linux下，打开Chrome-Setting-HTTPS/SSL-Manage certificates..., 选择Authorities这一栏，导入证书文件即可。 HTTPS代理抓包效果 如果代理失败，可能是Burpsuite版本太低，ssl默认为RC4算法，百度https不支持。 Burpsuite的功能非常强大，以后慢慢学习。]]></content>
      <categories>
        <category>安全</category>
        <category>Web安全</category>
      </categories>
      <tags>
        <tag>Burpsuite</tag>
        <tag>Web安全</tag>
        <tag>渗透测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中try/except/else/finally的执行机制]]></title>
    <url>%2F2016%2F07%2F26%2Ftry-except-else-finally-in-Python%2F</url>
    <content type="text"><![CDATA[最近经常使用到try/except/else/finally全部子句，对于执行顺序不是很熟悉，这里作一下记录。 不含return的情况一般情况try: print 'try'except: print 'except'else: print 'else'finally: print 'finally' output:tryelsefinally try子句异常try: print 'try'/0except: print 'except'else: print 'else'finally: print 'finally' output:exceptfinally try/except子句同时异常try: print 'try'/0except: print 'except'/0else: print 'else'finally: print 'finally' output: finallyTraceback (most recent call last): File "E:\tmp\test.py", line 4, in &lt;module&gt; print 'except'/0TypeError: unsupported operand type(s) for /: 'str' and 'int' else子句异常try: print 'try'except: print 'except'else: print 'else'/0finally: print 'finally' output: tryfinallyTraceback (most recent call last): File &quot;E:\tmp\test.py&quot;, line 12, in &lt;module&gt; t() File &quot;E:\tmp\test.py&quot;, line 7, in t print &apos;else&apos;/0TypeError: unsupported operand type(s) for /: &apos;str&apos; and &apos;int&apos; finally子句异常try: print 'try'except: print 'except'else: print 'else'finally: print 'finally'/0 output:tryelseTraceback (most recent call last): File "E:\tmp\test.py", line 11, in &lt;module&gt; t() File "E:\tmp\test.py", line 9, in t print 'finally'/0TypeError: unsupported operand type(s) for /: 'str' and 'int' 总结 finally子句总会执行 除了try子句，其他子句中的异常都会抛出 包含return的情况众所周知，return会直接跳出函数，那么它和异常机制的优先级如何呢？ try子句returndef t(): try: print 'try' return except: print 'except' else: print 'else' finally: print 'finally't() output: tryfinally else子句没有执行！finally子句仍然执行！ finally子句returndef t(): try: print 'try'/0 return 0 except: print 'except'/0 return 1 else: print 'else' return 2 finally: print 'finally' return 3print t() output: finally3 函数的返回值是3，而不是try子句中设置的0，而且except子句的异常不见了。但是如果注释掉finally子句的return，就可以看见except中抛出的异常。 def t(): try: print 'try'/0 return 0 except: print 'except'/0 return 1 else: print 'else' return 2 finally: print 'finally' # return 3print t() output: finallyTraceback (most recent call last): File "E:\tmp\test.py", line 15, in &lt;module&gt; print t() File "E:\tmp\test.py", line 6, in t print 'except'/0TypeError: unsupported operand type(s) for /: 'str' and 'int' 这是为什么呢？其实官方文档里有讲： A finally clause is always executed before leaving the try statement, whether an exception has occurred or not. When an exception has occurred in the try clause and has not been handled by an except clause (or it has occurred in a except or else clause), it is re-raised after the finally clause has been executed. The finally clause is also executed “on the way out” when any other clause of the try statement is left via a break, continue or return statement. 也就是说except、esle子句中的异常在finally子句执行之后才会抛出，但是finally直接return了，所以我们就看不到异常了，而且会覆盖本身的return值。这种用法显然是不符合初衷的，所以不建议在finally语句中使用return、break、continue这些跳出的语句，应仅用作资源释放操作，否则会出现意想不到的结果。 总结 执行顺序 finally &gt; return &gt; else finally的return会抑制其他子句异常抛出，不建议使用 一个例子def example(id): db = get_db() try: with db.cursor() as cursor: sql = '''update xxxx set yyyy = yyyy + 1 where id = %s''' cursor.execute(sql, (id,)) db.commit() except Exception, e: app.logger.error(e, exc_info=True) db.rollback() else: # do sth else return True finally: db.close() 注意return的位置和finally子句的用法。]]></content>
      <categories>
        <category>开发</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持select和input的JS组件]]></title>
    <url>%2F2016%2F07%2F21%2F7-%E6%94%AF%E6%8C%81select%E5%92%8Cinput%E7%9A%84JS%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[其实一直以来都不喜欢写JS，但是工作中又不可避免的要写一些前台的用户交互，之前每次都是现写现查，以至于写了不少前端代码都没有积累。既然不能逃避，不如选择接受，从现在起开始有意识得积累一些前端经验。 JS作为一门语言，其设计品味不敢恭维，但是得益于多年在浏览器上的耕耘，无数前端工作者在其上面的工作令其大放异彩。从早期的JQuery到现在的AngularJS、ReactJS、NodeJS，JS自身的语义设计也随着ES6的发布变得越来越完善，也许JS未来会成为一门“真正”的好语言吧。 扯了那么多，现在进入正题。作为一个业余的JS开发者，自己开发组件显然不是一种好选择，在我碰到一个既可以让用户选择也能让用户自行输入的表单字段时陷入了困扰。在github上的搜索让我找到了select2.js这个组件，真的非常强大。 其中一个选项满足了我的业务： &lt;!-- 必要文件的包含 --&gt;&lt;script type="text/javascript" src="dist/js/select2.min.js"&gt;&lt;/script&gt;···&lt;!-- select 框 --&gt;&lt;select name="handler" class="form-control js-example-tags" multiple="multiple"&gt; &lt;option value="1"&gt;orange&lt;/option&gt; &lt;option value="2"&gt;purple&lt;/option&gt; &lt;option value="3"&gt;bule&lt;/option&gt;&lt;/select&gt;&lt;!-- 启用组件 --&gt;$(".js-example-tags").select2(&#123; tags: true&#125;) 效果： 更多用法：http://select2.github.io/examples.html#tags]]></content>
      <categories>
        <category>开发</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>Javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用体验]]></title>
    <url>%2F2016%2F07%2F01%2F6-Hexo%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[最早开始搭建个人博客的时候使用WordPress，光安装环境就折腾了许久，而且很多静态资源默认都是国外的，加载缓慢，替换起来也很麻烦。 于是后来就开始体验一些静态博客，从最知名的jekyll，到python开发的pelican，然后是go语言写的hugo，使用过程都不太满意，直到我遇见了Hexo &amp; Next。 Why Hexo其实我很早知道hexo，在github上面stars的数量也很高，但是年少的我对NodeJS带有偏见，以至于一路过来都没有尝试她。下面是我选择她的一些理由: 工具 文章生成速度 配置部署 主题 开发者 Jekyll/Pelican 慢 简单 多 外国人 Hugo 快 麻烦(go的很多代码被墙了) 较少 外国人 Hexo 快 简单 很多很好看（js毕竟是前端程序员的主力武器） 台湾 所以我放弃Pelican的理由是太慢了，放弃Hugo的理由是因为找不到我想要的主题，而且本地化支持不是很好。Hexo部署简单，性能也还不错，但是仅仅如此是不够。最重要的是因为Next。 Why NextNext是Github上面starts数最多的Hexo主题，最重要的是她是国人开发的，有着对本地化最完美的支持。 比如我们使用多说评论框，在Hugo上要去改写测试layouts文件，很容易出错，但是在Next里你只需要把你的duoshuoID配置在_config.yaml文件里就可以了。 此外她有丰富的国人编写的插件及简洁优美的外观，配合Nginx和国内JS、CSS的CDN，性能表现近乎完美。具体的介绍可以参考Next主页。 安装 HexoHexo是用Node开发的，所以我们需要先安装Node。 安装方式有： yum install nodejs nvm安装 官网 安装 推荐第一种，因为hexo对node没有特殊要求，采用最方便的方式安装即可。 node的包管理工具叫做npm，hexo就是用它安装，但是默认配置的npm是国外的，速度不稳定，所以建议修改为国内源: $ vim ~/.profile 添加 export NVM_NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node 到文件尾，然后执行source ~/.profile使配置生效。 然后安装hexo，一行搞定:$ npm install hexo-cli -g 部署Hexo设置你的博客$ hexo init blog$ cd blog 安装Next$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题，打开blog/_config.yaml， 找到 theme 字段，并将其值更改为 nexttheme: next 开启服务$ hexo server# 默认监听localhost:4000，可以通过-i参数指定ip(不能写0.0.0.0)# eg. hexo server -i `xxx.com` 这个时候访问localhost:4000就能看到博客默认的样子了，不过建议实际部署的时候不要用hexo直接作为web服务器，而用分发好的静态文件直接部署在nginx上比较好，因为Nginx作为服务器性能比Hexo强，资源占用少。hexo server只是在我们预览效果的时候使用。 其他命令： 创建文章 $ hexo new "Hello Hexo" 生成静态文件 $ hexo generate 分发静态文件 $ hexo deploy 静态文件还可以通过git直接部署在github上面，利用github搭建自己的静态博客。具体方法就不多说了。当然，本博客就是用Hexo &amp; Next搭建在Nginx之上 ^_^ 同步Hexo如果你只在一台电脑上写博客，你就不需要看这一部分。但是如果你有在多机上写博客的需求，那么解决Hexo同步的问题就很必要了。 Hexo的部署命令针对public里的静态文件，对于文章源文件和配置等没有同步功能，除非你每次写完博客都把源文件存储在一个公共的地方，要不然你没有办法接下来承接之前的文章继续写作。 我在网路上搜索了一下，一般有这么几个解决方法： 用DropBox同步源文件 把源文件存放在服务器上 使用hexo-git-backup 使用git保存源代码(建议) 我本身的源文件就是存放在服务器上，所以不方便使用云来存储代码，hexo-git-backup这个插件对于太新的node也不支持，所以我选择使用git保存源代码。 使用git不仅能够解决同步问题，也可以解决回滚问题，而且也是作者建议的方式——如果你发现了目录里的.gitignore——你应该也会这么认为。 备份假设你把网站搭建在github上面，那么master分支保存静态文件，可以新建hexo分支保存源文件。 首先把本地代码提交到远端，确保远端仓库不存在hexo分支或者hexo分支为空。$ cd blog# 新建本地仓库$ git init# 新建并切换到hexo分支$ git checkout -b hexo# 本地提交$ git add .$ git commit -m 'init'# 配置远端仓库地址$ git remote add git@github.com:xxxx/xxxx.github.io.git# 远端提交$ git push origin hexo 至此同步配置已经结束，之后每次修改或者新增文件我们需要在本地（确保在hexo分支）提交源文件至远端hexo分支，然后运行hexo g -d即可。 恢复如果重装了电脑或者换了电脑，安装git、node、npm、hexo之后，依次执行以下命令即可:# 配置新电脑的sshkey到github，否则选择https方式clone$ git clone git@github.com:xxxx/xxxx.github.io.git -b hexo blog$ cd blog# 安装hexo依赖，确保之前安装hexo模块时没有漏掉--save选项# eg. npm install hexo-deployer-git --save$ npm install 无须执行hexo init。 弊端因为github上面repo是公开的，而源文件中会有一些不宜公开的数据配置，所以更保险的做法是使用私人的代码仓库保存源代码，比如收费的github private，或者码云。]]></content>
      <categories>
        <category>博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ngx_lua_waf搭建步骤]]></title>
    <url>%2F2016%2F06%2F30%2F5-Ngx-lua-waf%E6%90%AD%E5%BB%BA%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[Ngx_lua_waf是一个web防火墙，主要通过lua实现，安装简单，性能优异，由国人loveshell开发。 ngx_lua_waf是我刚入职趣游时候开发的一个基于ngx_lua的web应用防火墙。 代码很简单，开发初衷主要是使用简单，高性能和轻量级。 现在开源出来，遵从MIT许可协议。其中包含我们的过滤规则。如果大家有什么建议和想法，欢迎和我一起完善。 主页上介绍的安装流程不是很详细，我这里详细记录了一下。 安装openresty平台OpenResty 是一个基于 NGINX 和 LuaJIT 的 Web 平台,相当于集成了lua的Nginx，比自己编译安装Nginx+Lua要方便很多。步骤如下： # 切换至root用户su# 预装环境yum install readline-devel pcre-devel openssl-devel gcc# 下载安装包wget https://openresty.org/download/openresty-1.9.7.4.tar.gztar zxvf openresty-1.9.7.4.tar.gzcd openresty-1.9.7.4.tar.gz./configuremake &amp;&amp; make install 安装如果出现问题，可以参考 http://openresty.org/cn/installation.html 默认安装位置在/usr/local/openresty/nginx，修改conf/nginx.conf指定运行nginx worker的用户： user nginx;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid; 如果没有nginx用户，在root权限下运行useradd nginx即可添加。 如果是用systemctl管理服务，创建/lib/systemd/system/nginx.service文件，填写如下内容： [Unit]Description=nginxAfter=network.target[Service]Type=forkingExecStart=/usr/local/openresty/nginx/sbin/nginxExecReload=/usr/local/openresty/nginx/sbin/nginx -s reloadExecStop=/usr/local/openresty/nginx/sbin/nginx -s stopPrivateTmp=true[Install]WantedBy=multi-user.target 即可使用systemctl命令管理nginx服务： 启动 systemctl start nginx 关闭 systemctl stop nginx 重启 systemctl restart nginx 开启自启动 systemctl enable nginx 部署Ngx_lua_waf下载安装包到nginx配置目录，重命名为waf： cd /usr/local/openresty/nginx/confgit clone https://github.com/loveshell/ngx_lua_waf.git waf 然后修改Nginx.conf，在http下面增加以下waf配置： http &#123; include mime.types; default_type application/octet-stream; # Lua包的路径 lua_package_path "/usr/local/openresty/nginx/conf/waf/?.lua"; # Lua运行时最大内存 lua_shared_dict limit 10m; # Waf启动脚本 init_by_lua_file /usr/local/openresty/nginx/conf/waf/init.lua; # Waf过滤逻辑脚本 access_by_lua_file /usr/local/openresty/nginx/conf/waf/waf.lua; ··· 然后配置waf下的config.lua文件，主要是waf规则目录(RulePath)和日志目录(logdir): RulePath = "/usr/local/openresty/nginx/conf/waf/wafconf"--规则存放目录attacklog = "off"--是否开启攻击信息记录，需要配置logdirlogdir = "/usr/local/openresty/nginx/logs/hack/"--log存储目录，该目录需要用户自己新建，且需要nginx用户的可写权限UrlDeny="on"--是否拦截url访问Redirect="on"--是否拦截后重定向CookieMatch = "on"--是否拦截cookie攻击postMatch = "on" --是否拦截post攻击whiteModule = "on" --是否开启URL白名单black_fileExt=&#123;"php","jsp"&#125;--填写不允许上传文件后缀类型ipWhitelist=&#123;"127.0.0.1"&#125;--ip白名单，多个ip用逗号分隔ipBlocklist=&#123;"1.0.0.1"&#125;--ip黑名单，多个ip用逗号分隔CCDeny="on"--是否开启拦截cc攻击(需要nginx.conf的http段增加lua_shared_dict limit 10m;)CCrate = "100/60"--设置cc攻击频率，单位为秒.--默认1分钟同一个IP只能请求同一个地址100次html=[[Please go away~~]]--警告内容,可在中括号内自定义备注:不要乱动双引号，区分大小写 logdir目录尤其注意，需要用户自行创建，并且设置nginx用户可写，否则不会生成拦截日志也不会报错。 规则规则文件在wafconf下面，分为: args cookie post url user-agent whiteurl 文件一行一条规则，基于正则表达式编写，你可以根据你的需要进行修改。 效果在部署配置完成之后，执行命令systemctl restart nginx重启nginx服务，即可看到WAF效果： 拦截页面内容配置在config.lua的html字段中，你可以自定义更美观的样式。]]></content>
      <categories>
        <category>安全</category>
        <category>WAF</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
        <tag>WAF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现rsa的几种方案]]></title>
    <url>%2F2015%2F12%2F09%2F4-python%E5%AE%9E%E7%8E%B0rsa%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[rsa这是一个纯python实现的库，不依赖底层文件，优点是部署容易，缺点是速度比较慢，原本是斯坦福大学教学演示用的，现在由Sybren A. Stüvel专人维护。 生产环境不太建议使用此库。 解密性能: 70ms左右 Python-RSA is a pure-Python RSA implementation. It supports encryption and decryption, signing and verifying signatures, and key generation according to PKCS#1 version 1.5. It can be used as a Python library as well as on the commandline. The code was mostly written by Sybren A. Stüvel. pycrypto这个库应用的比较多，不局限于rsa，实现了大多数常用的加密算法。底层应该也是自己实现的，因为相比后面一个调用系统模块的库它的rsa算法要慢很多。 解密性能: 40ms左右 This is a collection of both secure hash functions (such as SHA256 and RIPEMD160), and various encryption algorithms (AES, DES, RSA, ElGamal, etc.). The package is structured to make adding new modules easy. This section is essentially complete, and the software interface will almost certainly not change in an incompatible way in the future; all that remains to be done is to fix any bugs that show up. If you encounter a bug, please report it in the Launchpad bug tracker at… M2Crypto这个库是对Openssl库接口的python封装，实际运算都是调用系统的libssl.so/libcryto.so，不过安装的时候比较麻烦，需要依赖swig等工具 (demo)。 解密性能: 7ms左右 ctypes+openssl以上列举的几种方法都有一个问题，就是不能利用多核。当我调用解密接口比较集中的时候，接口耗时会成倍增加，即便是最快的M2Crypto，瞬时10个并发的时候最坏情况就会达到70ms左右。 因此我尝试直接使用ctypes调用openssl库突破的GIL的限制，不过ctypes调用so文件有点麻烦的就是看不到so提供的函数名，反复查阅openssl的文档和参考c代码之后，最终调试成功。结果也非常令人满意，达到了1ms级，瞬时100个并发也能维持在毫秒级的性能。 解密性能: 1ms左右 import ctypesfrom ctypes.util import find_library_libcrypto = find_library('crypto')sign = 'iYzF0bn6kwUtsqLmTSx8fx...'cryptor = ctypes.cdll.LoadLibrary(_libcrypto)RSA_size = cryptor.RSA_sizeBIO_free = cryptor.BIO_freeRSA_free = cryptor.RSA_freeRSA_decrypt = cryptor.RSA_private_decryptPEM_read_bio_RSAPrivateKey = cryptor.PEM_read_bio_RSAPrivateKeyprivkey = '''-----BEGIN RSA PRIVATE KEY-----...-----END RSA PRIVATE KEY-----''' bio = cryptor.BIO_new_mem_buf(privkey, -1)key = PEM_read_bio_RSAPrivateKey(bio, 0, 0, 0)r = BIO_free(bio)if r != 1: # break here print 'BIO_free error' returnrsa_size = RSA_size(key)rsa = ctypes.create_string_buffer(rsa_size)ret = RSA_decrypt(rsa_size, sign, rsa, key, 1) 其中privkey是私钥，sign是我要解密的数据。 总结 加解密这一块，Linux下面能用openssl就用openssl，绝对比自己实现快。 python+ctypes可以让你享受python便利的同时还能拥有卓越的性能，不过ctypes创建的内存空间记得要自己释放，python垃圾收集机制对其无效。 一般情况下推荐用pycrypto库，文档比较全面。]]></content>
      <categories>
        <category>安全</category>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RSA</tag>
        <tag>密码学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用supervisor部署多个tornado服务]]></title>
    <url>%2F2015%2F12%2F09%2F3-%E5%88%A9%E7%94%A8supervisor%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AAtornado%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[单个Tornado服务由于文件句柄和处理请求数的限制不能够很好得满足实际的工作需求，因此我们搭建多个实例共同服务，但是Tornado自身没有这样的集群管理能力，所以我们需要借助第三方工具——Supervisor。Supervisor 是用Python编写的运行在Linux上的进程控制系统，用于监控和管理批量的服务进程，当前版本3.3.0。 安装$ pip install supervisor 配置$ echo_supervisord_conf &gt; /etc/supervisord.conf 修改默认配置编辑/etc/supervisord.conf： ; 修改unix socket文件位置，因为不清楚的原因，默认文件位置工作一段时间之后会不正常[unix_http_server]file=/home/libraco/conf/supervisor.sock ; (the path to the socket file)[supervisorctl]serverurl=unix:///home/libraco/conf/supervisor.sock ; use a unix:// URL for a unix socket; 取消注释，并修改配置文件目录[include]files = etc/supervisor/*.ini 配置Tornado在/etc/supervisor/下添加tornado.ini文件: [group:tornadoes]programs=tornado-8000,tornado-8001,tornado-8002,tornado-8003[program:tornado-8000];*命令路径,如果使用python启动的程序应该为 python /home/test.py, ;不建议放入/home/user/, 对于非user用户一般情况下是不能访问command=python /var/www/main.py --port=8000;执行目录,若有/home/supervisor_test/test1.py;将directory设置成/home/supervisor_test;则command只需设置成python test1.py;否则command必须设置成绝对执行目录directory=/var/www;*以www-data用户执行user=www-data;如果是true,当supervisor启动时,程序将会自动启动autostart=true;*自动重启autorestart=true;标准输出重定向到文件，标准错误重定向到标准输出stdout_logfile= /var/log/tornado-8000.logredirect_stderr=true[program:tornado-8001]command=python /var/www/main.py --port=8001directory=/var/wwwuser=www-dataautostart=trueautorestart=truestdout_logfile= /var/log/tornado-8001.logredirect_stderr=true[program:tornado-8002]command=python /var/www/main.py --port=8002directory=/var/wwwuser=www-dataautostart=trueautorestart=truestdout_logfile= /var/log/tornado-8002.logredirect_stderr=true[program:tornado-8003]command=python /var/www/main.py --port=8003directory=/var/wwwuser=www-dataautostart=trueautorestart=truestdout_logfile= /var/log/tornado-8003.logredirect_stderr=true 这里我们定义了一个名为tornadoes的组，包含四个成员tornado-8000、tornado-8001、tornado-8002、tornado-8003。 program定义进程名，每个program有单独的详细配置。 至此我们的配置工作基本完成。 启动$ /usr/bin/supervisord -c /etc/supervisord.conf 开机自启动$ sudo vim /lib/systemd/system/supervisor.service 写入如下内容:[Unit]Description=supervisorAfter=network.target[Service]Type=forkingExecStart=/usr/bin/supervisord -c /etc/supervisord.confPrivateTmp=true[Install]WantedBy=multi-user.target # 启动服务$ sudo systemctl start supervisor# 开机自启动$ sudo systemctl enable supervisor 管理服务管理服务需要用到supervisorctl命令，这个命令必须在Supervisord启动之后执行，它通过发送指令给Supervisord达到管理的目的。 常用的指令有： supervisorctl reload 重启Supervisord服务 supervisorctl restart &lt;name&gt;|&lt;gname&gt;重启某个服务或某一组服务 supervisorctl status查看服务运行的状态和时间 其他如果Tornado集群部署在Nginx反向代理之后，要获取到远程真实IP，除了必要的Nginx配置之外，Tornado也需要明确指定xheaders=True,官方有说明： If xheaders is True, we support the X-Real-Ip/X-Forwarded-For and X-Scheme/X-Forwarded-Proto headers, which override the remote IP and URI scheme/protocol for all requests. These headers are useful when running Tornado behind a reverse proxy or load balancer. The protocol argument can also be set to https if Tornado is run behind an SSL-decoding proxy that does not set one of the supported xheaders.具体代码示例： application = tornado.web.Application([ (r"/", MainHandler),])application.listen(options.port, '127.0.0.1', xheaders=True)]]></content>
      <categories>
        <category>运维</category>
        <category>Supervisor</category>
      </categories>
      <tags>
        <tag>Supervisor</tag>
        <tag>Tornado</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos使用Yum安装配置Mongodb]]></title>
    <url>%2F2015%2F12%2F03%2F2-Centos%E4%BD%BF%E7%94%A8Yum%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEMongodb%2F</url>
    <content type="text"><![CDATA[安装Mongodb使用Yum，我们可以很方便的安装Mongodb: yum install mongodb mongodb-server yum会自动帮我们生成Mongodb的配置文件，其中最主要的配置文件/etc/mongod.conf部分内容如下： # Comma separated list of ip addresses to listen on (all local ips by default)bind_ip = 127.0.0.1# Specify port number (27017 by default)#port = 27017# Fork server process (false by default)fork = true# Full path to pidfile (if not set, no pidfile is created)pidfilepath = /var/run/mongodb/mongod.pid# Log file to send write to instead of stdout - has to be a file, not directorylogpath = /var/log/mongodb/mongod.log# Alternative directory for UNIX domain sockets (defaults to /tmp)unixSocketPrefix = /var/run/mongodb# Directory for datafiles (defaults to /data/db/)dbpath = /var/lib/mongodb# Enable/Disable journaling (journaling is on by default for 64 bit)#journal = true#nojournal = true 默认配置指定了IP、端口、数据文件、日志文件等等，十分详尽，可以根据自己的实际情况进行修改，一般用默认的就行了。 配置开机自启动打开/etc/rc.d/rc.local文件，追加/usr/bin/mongod –config /etc/mongod.conf至行尾，保存即可。 尝试启动service mongod start 结果失败了，提示我使用指令systemctl status mongodb查看原因，指令输出类似这样: mongodb.service - High-performance, schema-free document-oriented database Loaded: loaded (/usr/lib/systemd/system/mongodb.service; enabled) Active: failed (Result: exit-code) since Mi 2013-04-03 15:48:01 EEST; 3min 13s ago Process: 1756 ExecStart=/usr/bin/mongod --quiet --config /etc/mongodb.conf (code=exited, status=14)apr 03 15:48:01 echelon mongod[1756]: /usr/lib/libstdc++.so.6(_ZNSt6localeC1EPKc+0x71b) [0x7f60471c952b]apr 03 15:48:01 echelon mongod[1756]: /usr/lib/libboost_filesystem.so.1.53.0(_ZN5boost10filesystem4path7codecvtEv+0x4f) [0x7f6047adfb6f]apr 03 15:48:01 echelon mongod[1756]: /usr/lib/libboost_filesystem.so.1.53.0(_ZNK5boost10filesystem4path14root_directoryEv+0x114) [0x7f6047ae1344]apr 03 15:48:01 echelon mongod[1756]: /usr/lib/libboost_filesystem.so.1.53.0(_ZN5boost10filesystem8absoluteERKNS0_4pathES3_+0x3e) [0x7f6047add16e]apr 03 15:48:01 echelon mongod[1756]: /usr/bin/mongod(_ZN5mongo27initializeServerGlobalStateEb+0xf3) [0x94fc73]apr 03 15:48:01 echelon mongod[1756]: /usr/bin/mongod(main+0x234) [0x7591f4]apr 03 15:48:01 echelon mongod[1756]: /usr/lib/libc.so.6(__libc_start_main+0xf5) [0x7f60468b7a15]apr 03 15:48:01 echelon mongod[1756]: /usr/bin/mongod() [0x76bcd5]apr 03 15:48:01 echelon systemd[1]: mongodb.service: main process exited, code=exited, status=14/n/aapr 03 15:48:01 echelon systemd[1]: Unit mongodb.service entered failed state 仍然看不出哪里有问题，直到google到了这样的字眼sudo chown -R mongodb: /var/{lib,log}/mongodb，恍然大悟。 使用Yum安装Mongodb会默认创建mongo用户,该用户的家目录是/var/lib/mongodb, 但是日志文件/var/log/mongodb/mongod.log和进程ID文件/var/run/mongodb/mongod.pid在其他的目录下面，这些目录属主不是mongodb用户，所以写入的时候会报权限问题。 修改目录/var/log/mongodb和/var/run/mongodb属主为monodb即可。 总结Yum安装软件十分简单，但是由于Mongodb的安装包忽略了权限问题，而且出错日志十分不明显，导致我花费数十分钟才解决。 其实Linux下权限问题十分常见，发生问题不知道原因的时候都可以往这方面尝试一下。 更新后来想了一下，应该不是安装包的问题，这问题未免也太低级了。 极有可能是我安装好的时候使用自己的帐号运行过服务，所以产生的配置文件及目录的属主是我平常用的帐号，因此mongodb这个用户没有写入和执行权限。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置多域名反向代理]]></title>
    <url>%2F2015%2F12%2F03%2F1-Nginx%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%9F%9F%E5%90%8D%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Nginx是一款用C语言编写的高性能Web服务器，常常架设在其他Web服务的外层，用于负载均衡、缓存静态文件、反向代理等。所谓反向代理，就是部署在服务器端转发请求的代理服务，用户请求通过它到达真正的后端资源。与此对应的“正向”代理，是部署在客户端的代理服务，对外的网络请求实际由它发出。打个比方，正向代理就像是你叫你儿子帮你去打酱油，你儿子就是一个正向代理。反向代理就是你去酱油店买酱油，酱油实际上是老板找隔壁借来再卖给你，你并不知情，此时酱油店老板就是一个反向代理。从这个比方中也很好理解，正向代理对请求方是可知的，而反向代理对于请求方来说一般是透明的、不可知的。 安装Nginx使用Yum，我们可以很方便的安装Nginx(apt-get是完全类似的): yum install nginx 打开nginx的配置文件/etc/nginx/nginx.conf，长这样: # For more information on configuration, see:# * Official English Documentation: http://nginx.org/en/docs/# * Official Russian Documentation: http://nginx.org/ru/docs/user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125;&#125; 配置反向代理在/etc/nginx/conf.d/目录下新建一个配置文件reverse_proxy.conf，输入以下内容: upstream wx_server &#123; server localhost:8001;&#125;proxy_temp_path /etc/nginx/proxy_temp;proxy_cache_path /etc/nginx/proxy_cache levels=1:2 keys_zone=cache_one:100m inactive=1d max_size=1g;server &#123; listen 80; server_name blog.hustlibraco.com hustlibraco.com; location ~ .*\.(gif|jpg|png|css|js|ico|swf)(.*) &#123; proxy_pass http://localhost:8000; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_cache cache_one; proxy_cache_valid 200 304 5m; proxy_cache_key $host$uri$is_args$args; expires 30d; &#125; location / &#123; proxy_pass http://localhost:8000; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; access_log /var/log/nginx/blog_hustlibraco_com_access.log;&#125;server &#123; listen 80; server_name wx.hustlibraco.com; location / &#123; proxy_pass http://wx_server; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; access_log /var/log/nginx/wx_hustlibraco_com_access.log;&#125; 其中两段server开头的配置指明了代理的规则，我们逐一解释下这些配置: listen 80监听80端口 server_name blog.hustlibraco.com hustlibraco.com接受这两个域名的请求 location ~ .\*\.(gif|jpg|png|css|js|ico|swf)(.\*){}指定文件的缓存配置 location / {}接受server_name下所有路径的请求 proxy_pass http://localhost:8000转发到localhost的8000端口上 proxy_pass http://wx_server转发到wx_server节点上，由upstream配置 proxy_redirect off不重写被代理服务器返回给客户端的Location和Refresh proxy_set_header Host $host设置请求头的Host为反向代理服务器的Host proxy_set_header X-Real-IP $remote_addr设置请求头的X-Real-IP为客户端真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for把请求来源的IP添加到请求头的X-Forwarded-For字段 X-Forwarded-For:简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，只有在通过了HTTP代理或者负载均衡服务器时才会添加该项。 它不是RFC中定义的标准请求头信息，在squid缓存代理服务器开发文档中可以找到该项的详细介绍。 标准格式如下：X-Forwarded-For: client1, proxy1, proxy2。 access_log /var/log/nginx/blog_hustlibraco_com_access.log请求日志文件 了解这些配置项的意义之后，可以知道发送给blog.hustlibraco.com, hustlibraco.com的请求都会被转发到本地8000服务上，而发送给wx.hustlibraco.com的请求被转发到wx_server节点上（localhost:8001）。 配置好以后使用命令service nginx reload使配置生效。 设置开机自启动CentOS7引入了systemctl命令，结合了service和chkconfig的功能，一行命令设设置开机自启动: systemctl enable nginx.serviceln -s '/usr/lib/systemd/system/nginx.service' '/etc/systemd/system/multi-user.target.wants/nginx.service']]></content>
      <categories>
        <category>运维</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
</search>